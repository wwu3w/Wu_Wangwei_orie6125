{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.11 (default, Dec  6 2015 18:57:58)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.4-src.zip'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DatasetRaw = sc.textFile('114_congress.csv')\n",
    "\n",
    "def remove_header(itr_index, itr):\n",
    "    return iter(list(itr)[1:]) if itr_index == 0 else itr\n",
    "\n",
    "DatasetRaw = DatasetRaw.mapPartitionsWithIndex(remove_header).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DatasetRaw = DatasetRaw.map(lambda line: [str(i + 1) + \":\" + x + \" \" for i, x in enumerate(line.split(',')[3:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DatasetRaw = DatasetRaw.zipWithIndex().map(lambda line: [str(line[1]) + ' '] + line[0]).map(lambda line: (1,line + ['\\n'])).map(lambda line: (line[0],''.join(line[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NewDataset = DatasetRaw.reduceByKey(lambda v1, v2: v1+v2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding = 'utf-8'\n",
    "import io\n",
    "\n",
    "with io.open('new_votes.txt', 'w', encoding=encoding) as file:\n",
    "    file.write(NewDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "\n",
    "#Kmeans\n",
    "dataset = spark.read.format(\"libsvm\").load(\"new_votes.txt\")\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PCA\n",
    "standardizer = StandardScaler(withMean=True, withStd=True, inputCol='features', outputCol='features_2')\n",
    "model = standardizer.fit(dataset)\n",
    "dataset = model.transform(dataset)\n",
    "pca = PCA(k=2, inputCol='features_2', outputCol='pcaFeatures')\n",
    "model = pca.fit(dataset)\n",
    "result = model.transform(dataset).select('pcaFeatures')\n",
    "result.show(truncate=False)\n",
    "\n",
    "# Putting all together\n",
    "predictions = model.transform(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
