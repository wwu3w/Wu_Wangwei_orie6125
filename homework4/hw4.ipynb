{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named model_selection",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-66f19fdbc829>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_mldata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named model_selection"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.extmath import safe_sparse_dot, squared_norm\n",
    "from scipy.misc import comb, logsumexp \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "X = mnist.data.astype('float64')\n",
    "y = mnist.target\n",
    "\n",
    "\n",
    "train_samples = 30000\n",
    "\n",
    "random_state = check_random_state(0)\n",
    "\n",
    "permutation = random_state.permutation(X.shape[0]) \n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_samples, test_size=10000, random_state=1)\n",
    "\n",
    "#Normalize Data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "Y_train = np.zeros((len(y_train), 10))\n",
    "for i,j in enumerate(y_train):\n",
    "    Y_train[i, int(j)] = 1\n",
    "Y_test = np.zeros((len(y_test), 10))\n",
    "for i,j in enumerate(y_test):\n",
    "    Y_test[i, int(j)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining a loss function\n",
    "def loss_function(X,Y, w, alpha=0):\n",
    "    n_classes = Y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "    w = w.reshape(n_classes, -1)\n",
    "    fit_intercept = w.size == (n_classes * (n_features + 1))\n",
    "    old_w = w.copy()\n",
    "    if fit_intercept:\n",
    "        intercept = w[:, -1]\n",
    "        w = w[:, :-1]\n",
    "    else:\n",
    "            intercept = 0\n",
    "    p = safe_sparse_dot(X, w.T)\n",
    "    p += intercept\n",
    "    p -= logsumexp(p, axis=1)[:, np.newaxis]\n",
    "    loss = -(Y * p).sum()\n",
    "    loss += 0.5 * alpha * squared_norm(w)\n",
    "    p = np.exp(p, p)\n",
    "    return loss, p, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_loss(X,Y, w, alpha=0):\n",
    "    # X shape: n_sample*n_feature\n",
    "    # Y shape: n_sample*n_class indicator matrix\n",
    "    # w shape: n_class*n_feature\n",
    "    n_classes = Y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "    w = w.reshape(n_classes, -1)\n",
    "    fit_intercept = w.size == (n_classes * (n_features + 1))\n",
    "    old_w = w.copy()\n",
    "    if fit_intercept:\n",
    "        intercept = w[:, -1]\n",
    "        w = w[:, :-1]\n",
    "    else:\n",
    "        intercept = 0\n",
    "            \n",
    "    b = safe_sparse_dot(X, w.T) # shape: n_sample*n_class\n",
    "    b += intercept\n",
    "    p1 = np.exp(b)\n",
    "    p2 = p1./np.exp(b, axis=1).sum()[:, np.newaxis] #x_i*exp(x_i\\dot w_k)/\\sum_k'{exp(x_i\\dot w_k')}\n",
    "    #shape: n_sample*n_class \n",
    "    grad = safe_sparse_dot(X.T, Y) - safe_sparse_dot(X.T, p2) #shape: n_feature*n_class\n",
    "    grad += alpha * w.T\n",
    "    grad = grad.T\n",
    "    return grad #shape: n_class*n_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sgd_skl(X_train, Y_train, momentum=0.9, lr=0.01, batch_size=1001, alpha=0.1, maxepoch=50, eps=1e-8):\n",
    "    \n",
    "    #Batch\n",
    "    Index1 = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(Index1)\n",
    "    n_batch = int(X_train.shape[0]/batch_size)\n",
    "    start_ind = np.arange(n_batch)*batch_size\n",
    "    end_ind = (np.arange(n_batch)+1)*batch_size+1\n",
    "\n",
    "    mu = list()\n",
    "    eta = list()\n",
    "    w = list()\n",
    "\n",
    "    mu.append(lr) \n",
    "    eta.append(momentum) \n",
    "    deltalambda = list()\n",
    "\n",
    "    #Initialization\n",
    "    ite = 0 # No. interation\n",
    "    inds = Index1[start_ind[ite]:end_ind[ite]]\n",
    "    \n",
    "    model = list()\n",
    "    model.append(SGDClassifier(loss='log', penalty=\"l2\", learning_rate='constant', eta0=mu[ite], alpha=eta[ite], epsilon=eps))\n",
    "    model[ite].fit(X_train[inds], Y_train[inds]) \n",
    "    w.append(model[ite].coef_)\n",
    "    mean_accuracy = list()\n",
    "    mean_accuracy.append(model[ite].score(X_train[inds], Y_train[inds]))\n",
    "\n",
    "    #Interations\n",
    "    #Each interation processes a single batch\n",
    "    \n",
    "    for ite in np.arange(n_batch): \n",
    "\n",
    "        inds = Index1[start_ind[ite]:end_ind[ite]]\n",
    "\n",
    "        #Phi(s_t,lam_t)\n",
    "        model.append(SGDClassifier(loss='log', penalty=\"l2\", learning_rate='optimal', eta0=mu[ite], alpha=eta[ite], epsilon=eps))      \n",
    "        model[ite+1].fit(X_train[inds], Y_train[inds])\n",
    "        \n",
    "        deltaphi = model[ite+1].score(X_train[inds], Y_train[inds]) - model[ite].score(X_train[inds], Y_train[inds])\n",
    "\n",
    "        w.append(model[ite+1].coef_)\n",
    "        deltaw = w[ite+1] - w[ite]\n",
    "\n",
    "        params = model[ite+1].get_params(deep=True) \n",
    "        mu.append(params['eta0'])\n",
    "        eta.append(params['alpha'])\n",
    "        deltalambda.append([mu[ite+1] - mu[ite], eta[ite + 1] - eta[ite]])\n",
    "        \n",
    "        #print state_t[ite+1].get_params(deep=True)['alpha']\n",
    "        #print state_t[ite+1].get_params(deep=True)['eta0']\n",
    "\n",
    "        mean_accuracy.append(clf[ite+1].score(X_train[inds], Y_train[inds]))\n",
    "        print(mean_accuracy[ite+1])\n",
    "        error = 1-mean_accuracy[ite]\n",
    "        v[ite+1] = mu[ite]*v[ite] + grad_J(w[ite+1], w[ite])\n",
    "        w[ite+1] = w[ite] - eta[ite]*(mu[ite]*v[ite] - grad_J(w[ite+1], w[ite]))\n",
    "\n",
    "    return mu, eta, w, state_t, mean_accuracy\n",
    "\n",
    "\n",
    "out = sgd_skl(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
